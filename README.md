# Real-time-Sign-Language-Translator
This program is developed to solve the communication barrier between Normal person and specially abled using Deep learning . This Application can be used as virtual camera in any video conference application.

## **PROJECT TITLE** :
### _Real-Time Communication System Powered by AI for Specially Abled_

![RTCS](https://user-images.githubusercontent.com/90745606/204100434-34d345e7-5a26-4895-9773-9c4ac85ab9c6.png)

<hr>

## üë©‚Äçüíª TEAM ID : PNT2022TMID22290
#### TEAM DETAILS :
**TEAM LEADER** &nbsp;&nbsp;- _Devanand M_<br>
**TEAM MEMBER** - _Dhinesh M_<br>
**TEAM MEMBER** - _Komesh S_<br>
**TEAM MEMBER** - _Veluru Balaji_<br>

<hr>

## üìÑ PROJECT DETAILS 
#### PROJECT DESCRIPTION :
&nbsp;&nbsp;&nbsp;&nbsp;In our society, we have people with disabilities. The technology is developing day by day but no significant developments are undertaken for the betterment of these people. Communications between deaf-mute and a normal person has always been a challenging task. It is very difficult for mute people to convey their message to normal people. Since normal people are not trained on hand sign language. In emergency times conveying their message is very difficult. The human hand has remained a popular choice to convey information in situations where other forms like speech cannot be used. Voice Conversion System with Hand Gesture Recognition and translation will be very useful to have a proper conversation between a normal person and an impaired person in any language.

&nbsp;&nbsp;&nbsp;&nbsp;The project aims to develop a system that converts the sign language into a human hearing voice in the desired language to convey a message to normal people, as well as convert speech into understandable sign language for the deaf and dumb. We are making use of a convolution neural network to create a model that is trained on different hand gestures. An app is built which uses this model. This app enables deaf and dumb people to convey their information using signs which get converted to human-understandable language and speech is given as output.

#### TECHNICAL ARCHITECTURE :
![modelCNN](https://user-images.githubusercontent.com/90745606/202712666-61999119-10e6-45dd-a7a8-7d234a2debf0.png)

<hr>

## üíæ INSTALLATION & OUTPUT PREVIEW 
### üíø INSTALLATION :
#### To install required libraries :
```ps
pip install -r requirements.txt
```
#### To install pytorch :
```ps
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
```
### To install Nvidia CUDA and CNNtoolkit
```ps
Download form the official site
```
#### To run the flask app :
```ps
python app.py
```
#### To run the virtual camera :
```ps
python SignCam.py
```
##### Note : You have to install OBS software to utilize the functions of virtual sign language camera.

### üì§ OUTPUT PREVIEW :

![ezgif com-gif-maker](https://user-images.githubusercontent.com/90745606/202739626-e46a25ea-a6d3-44d0-933e-6429f41413d1.gif)
<br>

##### DEMO-SITE : <a href="https://deva0813.github.io/rtcsys/">Link</a>&nbsp;(This is just a demo site and the video translate is an illustration of actual convertion and other features works properly.)

#### Demo Video : <a href="https://www.youtube.com/watch?v=O6DDeSUXrNs">Link</a>

